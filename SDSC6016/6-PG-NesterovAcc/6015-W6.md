
# Chapter 6: Proximal Gradient Descent and Nesterov's Acceleration
## Review
Consider the problem $$
\min _{x} f(x)
$$
with $f$ convex, and $\operatorname{dom}(f)=\mathbb{R}^{n}$.
 **<font color=black>Subgradient method</font>**:
* Choose an initial $x^{(0)} \in \mathbb{R}^{n}$
* Repeat:
$$
x^{(k)}=x^{(k-1)}-t_{k} \cdot g^{(k-1)}, \quad k=1,2,3, \ldots
$$
where $g^{(k-1)} \in \partial f\left(x^{(k-1)}\right) .$
Use pre-set rules for the step sizes (e.g., fixed step sizes rule, diminshing step sizes rule $t_k \propto 1/\sqrt{k}$)

> * If $f$ is Lipschitz, then subgradient method has a convergence rate $\frac{1}{\sqrt{k}}$, and thus the complexity  $O\left(1 / \epsilon^{2}\right)$
> * Upside: very generic.
> *  Downside: can be slow.


## Composite functions
Suppose the function to minimize takes the form  $$
f(x)=g(x)+h(x)
$$ where
- $g$ is convex, differentiable, $\operatorname{dom}(g)=\mathbb{R}^{n}$
- $h$ is convex, *not necessarily differentiable*.
**If $f$ were differentiable**, then gradient descent update would be:
$$
x^{+}=x-t \cdot \nabla f(x)
$$
> Minimize quadratic approximation to $f$ around $x,$ replace $\nabla^{2} f(x)$ by $\frac{1}{t} I$
$$
x^{+}=\underset{z}{\operatorname{argmin}}  \,\underbrace{f(x)+\nabla f(x)^{T}(z-x)+\frac{1}{2 t}\|z-x\|_{2}^{2}}_{\bar{f}_{t}(z)}
$$

**If $f$ is not differentiable, but $f=g+h, g$ differentiable**. Consider the quadratic approximation to $g$ but leave $h$ alone.

>That is, update
$$
\begin{aligned}
x^{+} &=\underset{z}{\operatorname{argmin}} \bar{g}_{t}(z)+h(z) \\
&=\underset{z}{\operatorname{argmin}} ~ g(x)+\nabla g(x)^{T}(z-x)+\frac{1}{2 t}\|z-x\|_{2}^{2}+h(z) \\
&=\underset{z}{\operatorname{argmin}} \frac{1}{2 t}\|z-(x-t \nabla g(x))\|_{2}^{2}+h(z)
\end{aligned}
$$
Note $x-t \nabla g(x)$ is the one-step gradient descent only for function $g$.


# Proximal gradient descent
Define ***proximal mapping***:
$$
\operatorname{prox}_{h, t}(x)=\underset{z}{\operatorname{argmin}} \frac{1}{2 t}\|x-z\|_{2}^{2}+h(z)
$$
>  If $h$ is convex and differential, then let $z_t=\operatorname{prox}_{h, t}(x)$ and $z_t$ satisfies $$\frac{z_t-x}{t}+\nabla h(z_t)=0, \quad z_t=x+t\nabla h(z_t)$$ Let $t\to 0+$, then $z_t\to x$ and $$\lim_{t\to 0} \frac{x-z_t}{t}=\nabla h(x)$$

**Proximal gradient descent**:
* Choose initialize $x^{(0)}$
* Repeat:
$$
x^{(k)}=\operatorname{prox}_{h, t_{k}}\left(x^{(k-1)}-t_{k} \nabla g\left(x^{(k-1)}\right)\right), \quad k=1,2,3, \ldots
$$

To make this update step look familiar, we can rewrite it as
$$
x^{(k)}=x^{(k-1)}-t_{k} \cdot G_{t_{k}}\left(x^{(k-1)}\right)
$$
where $G_{t}$ can be seen as  the *generalized gradient* of $f$:
$$
G_{t}(x)=\frac{x-\operatorname{prox}_{h, t}(x-t \nabla g(x))}{t}
$$

>$$\begin{aligned} x^{(k-1)}-t_{k} \cdot G_{t_{k}}\left(x^{(k-1)}\right)=\operatorname{prox}_{h, t_{k}}\left(x^{(k-1)}-t_{k} \nabla g\left(x^{(k-1)}\right)\right)\\
 G_{t_{k}}\left(x^{(k-1)}\right)=\frac{x^{(k-1)}-\operatorname{prox}_{h, t_{k}}\left(x^{(k-1)}-t_{k} \nabla g\left(x^{(k-1)}\right)\right)}{t_{k}}\end{aligned}$$
>Show that  if $h$ is convex and differential, then as $t\to 0+$, $G_t(x)\to \nabla f(x)=\nabla g(x) + \nabla h(x)$.

---

*  Mapping prox $_{h, t}(\cdot)$ doesn't depend on $g$ at all, only on $h$.
*  $\operatorname{prox}_{h, t}(\cdot)$ may have a **closed-form** for many important functions $h$ .
* Smooth part $g$ can be complicated, we only need to compute its gradients.



## Example: ISTA (iterative soft-thresholding algorithm) for lasso
Given $y \in \mathbb{R}^{n}, X \in \mathbb{R}^{n \times p},$ recall the lasso criterion:
$$
f(\beta)=\underbrace{\frac{1}{2}\|y-X \beta\|_{2}^{2}}_{g(\beta)}+\underbrace{\lambda\|\beta\|_{1}}_{h(\beta)}
$$
Recall that the proximal mapping now is
 $$
\begin{aligned}
\operatorname{prox}_{t}(\beta) &=\underset{z}{\operatorname{argmin}} \frac{1}{2 t}\|\beta-z\|_{2}^{2}+\lambda\|z\|_{1} \\
&=S_{\lambda t}(\beta)
\end{aligned}
$$where $S_{\lambda}(\beta)$ is the **soft-thresholding operator**
$$
\left[S_{\lambda}(\beta)\right]_{i}=\left\{\begin{array}{ll}
\beta_{i}-\lambda & \text { if } \beta_{i}>\lambda \\
0 & \text { if }-\lambda \leq \beta_{i} \leq \lambda, \quad i=1, \ldots, n \\
\beta_{i}+\lambda & \text { if } \beta_{i}<-\lambda
\end{array}\right.
$$


<center>
<img src="https://github.com/recluse04/filesave/blob/master/w6_1.png?raw=true" width=500>



Recall $\nabla g(\beta)=-X^{T}(y-X \beta),$ hence proximal gradient update is:
$$
\beta^{+}=S_{\lambda t}\left(\beta+t X^{T}(y-X \beta)\right)
$$


Often called the **<font color=red>iterative soft-thresholding algorithm (ISTA)</font>**[^ista].

This similar idea can be applied to any convex $\ell_1$-regularized problems.
<center>
<img src="https://github.com/recluse04/filesave/blob/master/w6_3_2.png?raw=true" height=400>



## Backtracking line search
Backtracking for proximal gradient descent works similar as in gradient descent, but operates on $g$ and not $f$.

Choose parameter $0<\beta<1 .$ At each iteration, start at $t=t_{\text {init }}$ and while
$$
g\left(x-t G_{t}(x)\right)>g(x)-t \nabla g(x)^{T} G_{t}(x)+\frac{t}{2}\left\|G_{t}(x)\right\|_{2}^{2}
$$
shrink $t=\beta t,$ for some $0<\beta<1$ .



# Convergence analysis
For criterion $f(x)=g(x)+h(x),$ where
 * $g$ is convex, differentiable, $\operatorname{dom}(g)=\mathbb{R}^{n},$ and $\nabla g$ is Lipschitz continuous with constant $L>0$.
 * $h$ is convex,.

Define  
$$ \operatorname{prox}_{t}(x) \triangleq  \operatorname{argmin}_{z}\left\{\|x-z\|_{2}^{2} /(2 t)+h(z)\right\}$$
which can be evaluated analytically.

$\fcolorbox{red}{aqua}{Proximal gradient descent has convergence rate}$: $\textcolor{blue}{O(1 / k)}$. 

 **Theorem**: Proximal gradient descent with fixed step size $t \leq$ $1 / L$ satisfies
$$
f\left(x^{(k)}\right)-f^{\star} \leq \frac{\left\|x^{(0)}-x^{\star}\right\|_{2}^{2}}{2 t k}
$$
and the same result holds for backtracking, with $t$ replaced by $\beta / L$.

**Proof**:
> Due to the convexity of $g$ and Lipschitz continuous of $\nabla g$, there holds **explain this!!**
> $$g(x^{(k+1)})+h(x^{(k+1)})\leq g(x^{(k)})+\langle x^{(k+1)}-x^{(k)}, \nabla g(x^{(k+1)}) \rangle+\frac{L}{2} \|x^{(k+1)}-x^{k}\|+h(x^{(k+1)})$$
>For any $z\in \mathbb{R}^n$, let $f(z)$ subtracts both sides,  we have
$$
\begin{aligned}
f(z)-(g(x^{(k+1)})+h(x^{(k+1)})) &\geq f(z)-(g(x^{k})+\langle x^{(k+1)}-x^{(k)}, \nabla g(x^{k}) \rangle\\
&+\frac{L}{2} \|x^{(k+1)}-x^{k}\|+h(x^{(k+1)}))  \tag{*}
\end{aligned}
$$
Consider the update of ISTA**are you proving ISTA or general $g$??**, according to the optimal condition of subgradient
$$0\in \nabla g(x^{(k)})+L(x^{(k+1)}-x^{(k)})+\partial h(x^{(k)})$$
when the optimal solution **???what???**is obtained.
Let $\gamma (x^{(k)})\in \partial h(x^{(k)})$ be such that
$$0= \nabla g(x^{(k)})+L(x^{(k+1)}-x^{(k)})+\gamma (x^{(k)})$$
Now, since $g, h$ are convex we have
$$
\begin{aligned}
g(z) &\geq g(x^{(k)})+\langle z-x^{(k)}, \nabla g(x^{(k)})\rangle \\
h(z) &\geq h(x^{(k+1)})+\langle z-x^{(k+1)}, \gamma (x^{(k)})\rangle
\end{aligned}
$$
Summing the above two inequalities yields
$$
f(z)\geq g(x^{(k)})+\langle z-x^{(k)}, \nabla g(x^{(k)})\rangle +h(x^{(k+1)})+\langle z-x^{(k+1)}, \gamma (x^{(k)})\rangle
$$
Bring it to $(*)$ gives
$$
\begin{aligned}
f(z)-f(x^{(k+1)}) & \geq-\frac{L}{2}\left\|x^{(k+1)}-x^{(k)}\right\|^{2}+\left\langle z-x^{(k+1)}, \nabla g(x^{(k)})+\gamma(x^{(k)})\right\rangle \\ 
&=-\frac{L}{2}\left\|x^{(k+1)}-x^{(k)}\right\|^{2}+L\left\langle z-x^{(k+1)}, x^{(k)}-x^{(k+1)}\right\rangle \\
&=\frac{L}{2}\left\|x^{(k+1)}-x^{(k)}\right\|^{2}+L\left\langle x^{(k)}-z, x^{(k+1)}-x^{(k)}\right\rangle (**)
\end{aligned}
$$
Let $z=x^\star$, we have
$$
\begin{aligned}
\frac{2}{L}\left(f(x^\star)-f(x^{k+1})\right) & \geq\left\|x^{(k+1)}-x^{(k)}\right\|^{2}+2\left\langle x^{(k)}-x^\star, x^{(k+1)}-x^{(k)}\right\rangle \\
&=\left\|x^{(k+1)}\right\|^2-\left\|x^{(k)}\right\|^2-2 \left\langle x^\star, x^{(k+1)}\right\rangle+2\left\langle x^\star,x^{(k)}\right\rangle \\
&=\left\|x^\star-x^{(k+1)}\right\|^{2}-\left\|x^\star-x^{(k)}\right\|^{2}
\end{aligned}
$$
Summing this inequality over $i=0, \ldots, k-1$ gives
$$
\frac{2}{L}\left(k f(x^\star)-\sum_{i=0}^{k-1} f(x^{(i)})\right) \geq\left\|x^\star-x^{(k)}\right\|^{2}-\left\|x^\star-x^{(0)}\right\|^{2} (***)
$$
Reconsider $(**)$
$$f(z)-f(x^{(k+1)})  \geq  \frac{L}{2}\left\|x^{(k+1)}-x^{(k)}\right\|^{2}+L\left\langle x^{(k)}-z, x^{(k+1)}-x^{(k)}\right\rangle$$
Let $z=x^{(k)}$
$$
\frac{2}{L}\left(f(x^{(k)})-f(x^{(k+1)})\right) \geq\left\|x^{(k)}-x^{(k+1)}\right\|^{2}
$$
Multiplying this inequality by $i$ and summing over $i=0, \ldots, k-1,$ we obtain
$$
\frac{2}{L} \sum_{i=0}^{k-1}\left(i f\left(x^{(i)}\right)-(i+1) f\left(x^{(i+1)}\right)+f\left(x^{(i+1)}\right)\right) \geq \sum_{i=0}^{k-1} i\left\|x^{(i)}-x^{(i+1)}\right\|^{2}
$$
$$
\frac{2}{L}\left(-k f\left(x^{(k)}\right)+\sum_{i=0}^{k-1} f(x^{(i+1)})\right) \geq  \sum_{i=0}^{k-1} i\left\|x^{(i)}-x^{(i+1)}\right\|^{2}
$$
Adding this inequality and $(***)$ , we get
$$
\frac{2 k}{L}\left(f\left(x^\star\right)-f\left(x^{(k)}\right)\right) \geq\left\|x^\star-x^{(k)}\right\|^{2}+\sum_{i=0}^{k-1} i\left\|x^{(k)}-x^{(k+1)}\right\|^{2}-\left\|x^\star-x^{(0)}\right\|^{2}
$$
and hence it follows that
$$
f\left(x^{(k)}\right)-f\left(x^\star\right) \leq \frac{L\left\|x^\star-x^{(0)}\right\|^{2}}{2 k}\leq\frac{\left\|x^\star-x^{(0)}\right\|_{2}^{2}}{2 t k}
$$

## Example: matrix completion
Given a matrix $Y \in \mathbb{R}^{m \times n}$, and only observe entries $Y_{i j},(i, j) \in \Omega$. Suppose we want to fill in missing entries (e.g., for a recommender system ).
<center><img src="https://upload.wikimedia.org/wikipedia/commons/b/b1/Rank-1-matrix-completion.png" width=400>

so we solve a matrix completion problem:
$$
\min _{B} \frac{1}{2} \sum_{(i, j) \in \Omega}\left(Y_{i j}-B_{i j}\right)^{2}+\lambda\|B\|_{\mathrm{tr}}
$$
Here $\|B\|_{\text {tr }}$ is the trace (or nuclear) norm of $B$,
$$
\|B\|_{\mathrm{tr}}=\sum_{i=1}^{r} \sigma_{i}(B)
$$
where $r=\operatorname{rank}(B)$ and $\sigma_{1}(X) \geq \cdots \geq \sigma_{r}(X) \geq 0$ are the singular values.


Define $P_{\Omega},$ the *projection operator* onto observed set:
$$
\left[P_{\Omega}(B)\right]_{i j}=\left\{\begin{array}{ll}
B_{i j} & (i, j) \in \Omega \\
0 & (i, j) \notin \Omega
\end{array}\right.
$$
Then **the matrix completition problem** is
$$
\min_{B\in \mathbb{R}^{m\times n}}f(B)=\underbrace{\frac{1}{2}\left\|P_{\Omega}(Y)-P_{\Omega}(B)\right\|_{F}^{2}}_{g(B)}+\underbrace{\lambda\|B\|_{\mathrm{tr}}}_{h(B)}
$$
where the F-norm is $\|A\|_F^2=\sum_{ij}A_{ij}^2$.

----


Two ingredients needed for proximal gradient descent:
* Gradient calculation: $\nabla g(B)=-\left(P_{\Omega}(Y)-P_{\Omega}(B)\right)$
* Prox function:
$$
\operatorname{prox}_{t}(B)=\underset{Z}{\operatorname{argmin}} \frac{1}{2 t}\|B-Z\|_{F}^{2}+\lambda\|Z\|_{\mathrm{tr}}
$$

**Claim**:
$$\operatorname{prox}_{t}(B)=S_{\lambda t}(B)$$ the ***matrix soft-thresholding*** at the level $\lambda$.
Here $S_{\lambda}(B)$ is defined by
$$
S_{\lambda}(B)=U \Sigma_{\lambda} V^{T}
$$
where $B=U \Sigma V^{T}$ is an SVD, and $\Sigma_{\lambda}$ is diagonal with
$$
\left(\Sigma_{\lambda}\right)_{i i}=\max \left\{\Sigma_{i i}-\lambda, 0\right\}
$$
>Proof: note that $\operatorname{prox}_{t}(B)=Z,$ where $Z$ satisfies
$$
0 \in Z-B+\lambda t \cdot \partial\|Z\|_{\mathrm{tr}}
$$
If $Z=U \Sigma V^{T}$, then
$$
\partial\|Z\|_{\mathrm{tr}}=\left\{U V^{T}+W:\|W\|_{\mathrm{op}} \leq 1, U^{T} W=0, W V=0\right\}
$$ where $\|W\|_{\mathrm{op}}=\sigma_{1}(W)$ is the operator/spectral norm of a matrix.
Now plug in $Z=S_{\lambda t}(B)$ and check that we can get 0.  

Hence proximal gradient update step is:
$$
B^{+}=S_{\lambda t}\left(B+t\left(P_{\Omega}(Y)-P_{\Omega}(B)\right)\right)
$$

Note that $\nabla g(B)$ is Lipschitz continuous with $L=1$, so we can choose fixed step size $t=1 .$ Update step is now:
$$
B^{+}=S_{\lambda}\left(P_{\Omega}(Y)+P_{\Omega}^{\perp}(B)\right)
$$
where $P_{\Omega}^{\perp}$ projects onto unobserved set, $P_{\Omega}(B)+P_{\Omega}^{\perp}(B)=B$
This is the **soft-impute algorithm**^[https://www.jmlr.org/papers/v11/mazumder10a.html].


## Special cases
Proximal gradient descent also called **<font color=red>composite gradient descent</font>** , or **<font color=red>generalized gradient descent</font>**.
 Here "generalized" refers to the several special cases:
 * $h=0:$ gradient descent
 * $h=I_{C}:$ projected gradient descent
 * $g=0$ : proximal minimization algorithm


## Projected gradient descent
Given closed, convex set $C \in \mathbb{R}^{n}$,
$$
\min _{x \in C} g(x) \Longleftrightarrow \min _{x} g(x)+I_{C}(x)
$$
where $I_{C}(x)=\left\{\begin{array}{ll}0 & x \in C \\ \infty & x \notin C\end{array}\right.$ is the indicator function of $C$
Hence
$$
\begin{aligned}
\operatorname{prox}_{t}(x) &=\underset{z}{\operatorname{argmin}} \frac{1}{2 t}\|x-z\|_{2}^{2}+I_{C}(z) \\
&=\underset{z \in C}{\operatorname{argmin}}\|x-z\|_{2}^{2}
\end{aligned}
$$
That is, $\operatorname{prox}_{t}(x)=P_{C}(x),$ projection operator onto $C$

Therefore proximal gradient update step is:
$$
x^{+}=P_{C}(x-t \nabla g(x))
$$
That is to perform usual gradient update and then project back onto $C$.  This is known as **projected gradient descent.**
<center>
<img src="https://github.com/recluse04/filesave/blob/master/w6_6.png?raw=true" width=400>


## Proximal minimization algorithm


*What happens if we can't evaluate the prox?*


Theory for proximal gradient, with $f=g+h$, assumes that prox function can be evaluated, i.e., assumes the minimization
$$
\operatorname{prox}_{t}(x)=\underset{z}{\operatorname{argmin}} \frac{1}{2 t}\|x-z\|_{2}^{2}+h(z)
$$
can be done exactly and quickly. In general, it is not clear what happens if we just minimize this approximately

But, if you can precisely control the errors in approximating the prox operator, then you may recover the original convergence rates.

In practice, if prox evaluation is done approximately, then it should be done to decently high accuracy.




# Nesterov's Accelerated Gradient Method


 Turns out we can accelerate proximal gradient descent in order to achieve the optimal $O(1 / \sqrt{\epsilon})$ convergence rate. Four ideas (three acceleration methods) by Nesterov:
- 1983: original acceleration idea for smooth functions
- 1988: another acceleration idea for smooth functions
- 2005: smoothing techniques for nonsmooth functions, coupled with original acceleration idea
- 2007: acceleration idea for composite functions

We will follow Beck and Teboulle (2008)[^ista], an extension of Nesterov (1983) to composite functions.


## Nesterov's (momentum) acceleration

Consider the minimization of a convex $f$ by the gradient descent method: $x^{(k)}=x^{(k-1)} -t_k \nabla f(x^{(k-1)})$.


In a seminal paper^[https://www.semanticscholar.org/paper/A-method-for-solving-the-convex-programming-problem-Nesterov/8d3a318b62d2e970122da35b2a2e70a5d12cc16f],  ***Nesterov*** proposed an ***accelerated gradient method*** (Nesterov, 1983)


$$\begin{array}{l}
x^{(k)}=v^{(k-1)}-t_k \nabla f\left(v^{(k-1)}\right) \\
v^{(k)}=x^{(k)}+\frac{k-1}{k+2}\left(x^{(k)}-x^{(k-1)}\right)
\end{array}
$$ with initial $x^{(0)}=v^{(0)}$.

For any fixed step size $\eta \leq 1 / L,$ where $L$ is the Lipschitz constant of $\nabla f,$ this scheme exhibits the convergence rate with constant step size $t$
$$
f\left(x_{k}\right)-f^{\star} \leq O\left(\frac{\left\|x_{0}-x^{\star}\right\|^{2}}{t k^{2}}\right)
$$
Above, $x^{\star}$ is any minimizer of $f$ and $f^{\star}=f\left(x^{\star}\right) .$
- It is well-known that this rate $O(1/k^2)$ is **optimal** among all methods having only information about the gradient of $f$ (i.e., *the first-order method*) at consecutive iterates (Nesterov, 2004).
- This is in contrast to vanilla gradient descent methods, which  can only achieve a rate of $O(1 / k) .$  
-  $v^{(k-1)}=x^{(k-1)}+\frac{k-2}{k+1}\left(x^{(k-1)}-x^{(k-2)}\right)$ carries some "**momentum**" from *previous* iterations. 
-  The sequence $\frac{k−1}{ k+2}=1-\frac{3}{k+2}$ (referred to as ”momentum weights”) is crucial for this convergence rate to hold.
- For small $k$, there is very little momentum in $v$ because the direction is uninformative, then the momentum weights become bigger as you approach the solution and the directions $v$ are more informative, which counteracts vanishing gradients near the solution.
-  As $k\to \infty,$  $\frac{k-1}{k+2}\to 1$ and $v^{(k)} \approx  2x^{(k)}-x^{(k-1)}$



## Continuos model of Nesterove's accelerated gradient method

+ The continuous model of gradient descent is the **gradient flow** (an ordinary differential equation)
 $$X'(t) = -\nabla f(X)$$
with $X(0)=x_0$
- In 2016, Su *et al* ^[https://jmlr.org/papers/v17/15-084.html] shows the continuos ODE for the Nesterove's accelerated gradient method takes the following form:
$$
\ddot{X}+\frac{3}{t} \dot{X}+\nabla f(X)=0
$$
for $t>0,$ with initial conditions $X(0)=x_{0}, \dot{X}(0)=0 ;$ here, $x_{0}$ is the starting point in Nesterov's scheme, $\dot{X} \equiv \mathrm{d} X / \mathrm{d}t$ denotes the time derivative or velocity .




## (Nesterov's) accelerated proximal gradient method
For the composite function
$$
\min _{x} g(x)+h(x)
$$
where $g$ convex, differentiable, and $h$ convex. Accelerated proximal gradient method: choose initial point $x^{(0)}=v^{(0)} \in \mathbb{R}^{n},$ repeat:
$$
\begin{aligned}
x^{(k)} &=\operatorname{prox}_{h,t_{k}}\left(v^{(k-1)}-t_{k} \nabla g(v^{(k-1)})\right) \\
v^{(k)} &=x^{(k)}+\frac{k-1}{k+2}\left(x^{(k)}-x^{(k-1)}\right)
\end{aligned}
$$
for $k=1,2,3, \ldots$

>* The first step $k=1$ is just usual proximal gradient update (i.e., $v^{(0)}=x^{(0)}$)
> *  When $h=0$ we get the Nesterov's accelerated gradient method.




---
Backtracking under with acceleration in different ways. Simple approach: fix $\beta<1, t_{0}=1 .$ At iteration $k,$ start with $t=t_{k-1}$, and while
$$
g\left(x^{+}\right)>g(v)+\nabla g(v)^{T}\left(x^{+}-v\right)+\frac{1}{2 t}\left\|x^{+}-v\right\|_{2}^{2}
$$
shrink $t=\beta t,$ and let $x^{+}=\operatorname{prox}_{t}(v-t \nabla g(v)) .$ Else keep $x^{+}$
> This strategy forces us to take decreasing step sizes.

## Convergence analysis
For criterion $f(x)=g(x)+h(x),$ we assume as before:
> * $g$ is convex, differentiable, $\operatorname{dom}(g)=\mathbb{R}^{n},$ and $\nabla g$ is lipschitz continuous with constant $L>0$.
> * $h$ is convex, $\operatorname{prox}_{t}(x)=\operatorname{argmin}_{z}\left\{\|x-z\|_{2}^{2} /(2 t)+h(z)\right\}$ can
be evaluated.

**Theorem**: Accelerated proximal gradient method with fixed step size $t \leq 1 / L$ satisfies $$ f\left(x^{(k)}\right)-f^{\star} \leq \frac{2\left\|x^{(0)}-x^{\star}\right\|_{2}^{2}}{t(k+1)^{2}} $$
> Proof: See Beck's paper[^ista].

[^ista]: https://epubs.siam.org/doi/abs/10.1137/080716542?mobileUi=0


Achieves the *optimal* convergence rate $O\left(1 / k^{2}\right)$ or the complexity $O(1 / \sqrt{\epsilon})$ for first-order methods.

## Example: ISTA+Nesterov's acceleration
Back to lasso problem:
$$
\min _{\beta} \frac{1}{2}\|y-X \beta\|_{2}^{2}+\lambda\|\beta\|_{1}
$$
Recall **<font color=red> ISTA</font>** (Iterative Soft-thresholding Algorithm):
$$
\beta^{(k)}=S_{\lambda t_{k}}\left(\beta^{(k-1)}+t_{k} X^{T}\left(y-X \beta^{(k-1)}\right)\right), \quad k=1,2,3, \ldots
$$
$S_{\lambda}(\cdot)$ being vector soft-thresholding.  Applying acceleration gives us ISTA with **<font color=red> Nesterov's acceleration</font>**:
For $k=1,2,3, \ldots$
$$
\begin{aligned}
v &=\beta^{(k-1)}+\frac{k-2}{k+1}\left(\beta^{(k-1)}-\beta^{(k-2)}\right) \\
\beta^{(k)} &=S_{\lambda t_{k}}\left(v+t_{k} X^{T}(y-X v)\right)
\end{aligned}
$$
<center><img src="https://github.com/recluse04/filesave/blob/master/w6_4_2.png?raw=true" width=500>

---
In practice the speedup of using acceleration is diminished in the presence of warm starts. For example, suppose want to solve lasso problem for tuning parameters values
$$
\lambda_{1}>\lambda_{2}>\cdots>\lambda_{r}
$$
> * When solving for $\lambda_{1},$ initialize $x^{(0)}=0,$ record solution $\hat{x}\left(\lambda_{1}\right)$.
> * When solving for $\lambda_{j},$ initialize $x^{(0)}=\hat{x}\left(\lambda_{j-1}\right),$ the recorded solution for $\lambda_{j-1}$.
> * Over a fine enough grid of $\lambda$ values, proximal gradient descent can often perform just as well without acceleration.

Sometimes backtracking and acceleration can be disadvantageous.
Recall matrix completion problem: the proximal gradient update is
$$
B^{+}=S_{\lambda}\left(B+t\left(P_{\Omega}(Y)-P^{\perp}(B)\right)\right)
$$
where $S_{\lambda}$ is the matrix soft-thresholding operator (requires $\mathrm{SVD}$).
One backtracking loop evaluates prox, across various values of
$t .$ For matrix completion, this means multiple SVDs.

> *  Acceleration changes argument we pass to prox: $v-t \nabla g(v)$ instead of $x-t \nabla g(x)$.

For matrix completion (and $t=1$).
$$
\begin{array}{c}
B-\nabla g(B)=\underbrace{P_{\Omega}(Y)}_{\text {sparse }}+\underbrace{P_{\Omega}^{\perp}(B)}_{\text {low rank }} \Rightarrow \text { fast } \mathrm{SVD} \\
V-\nabla g(V)=\underbrace{P_{\Omega}(Y)}_{\text {sparse }}+\underbrace{P_{\Omega}^{\perp}(V)}_{\text {not necessarily low rank}} \Rightarrow\text { slow SVD } \\
\end{array}
$$

## Homework
Consider  the proximal mapping
$$
\operatorname{prox}_{h, t}(x)=\underset{z}{\operatorname{argmin}} \frac{1}{2 t}\|x-z\|_{2}^{2}+h(z)
$$
Define the proximal  minimisation updates  
$$
x^{(k+1)}=\operatorname{prox}_{h, t}\left(x^{(k)}\right), \quad k=0,1,2,3, \ldots
$$
Write out these updates explicitly when applied to $h(x)=\frac{1}{2} x^{T} A x-b^{T} x,$ where $A \in \mathbb{S}^{n}$, a symmetric matrix.   Show that this is equivalent to the iterative refinement algorithm for solving the linear system $A x=b$ :
$$
x^{(k+1)}=x^{(k)}+(A+\epsilon I)^{-1}\left(b-A x^{(k)}\right), \quad k=1,2,3, \ldots
$$
where $\epsilon>0$ is some constant.  Write the expression of $\epsilon$. If $x^{(k)}$ converges to $x_*$ as $k\to \infty$,  what equation does $x_*$ satisfy.



## References and further reading
Nesterov's four ideas (three acceleration methods):
- Y. Nesterov (1983), "[A method for solving a convex programming problem with convergence rate $O\left(1 / k^{2}\right)$](http://mpawankumar.info/teaching/cdt-big-data/nesterov83.pdf)
- Y. Nesterov (1988), "[On an approach to the construction of optimal methods of minimization of smooth convex functions](https://link.springer.com/article/10.1007/s11590-015-0936-x) "
- Y. Nesterov (2005), "[Smooth minimization of non-smooth functions](https://link.springer.com/article/10.1007/s10107-004-0552-5)"
- Y. Nesterov (2007), "[Gradient methods for minimizing composite objective function](http://www.optimization-online.org/DB_FILE/2007/09/1784.pdf)"
Extensions and/or analyses:
- A. Beck and M. Teboulle (2008), "[A fast iterative shrinkage-thresholding algorithm for linear inverse problems](http://people.rennes.inria.fr/Cedric.Herzet/Cedric.Herzet/Sparse_Seminar/Entrees/2012/11/12_A_Fast_Iterative_Shrinkage-Thresholding_Algorithmfor_Linear_Inverse_Problems_(A._Beck,_M._Teboulle)_files/Breck_2009.pdf)"
- S. Becker and J. Bobin and E. Candes (2009), "[NESTA: a fast and accurate first-order method for sparse recovery](https://arxiv.org/pdf/0904.3367.pdf)"
P. Tseng (2008), "[On accelerated proximal gradient methods for convex-concave optimization](https://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf)"


Helpful lecture notes/books:
- E. Candes, [Lecture notes for Math 301](https://statweb.stanford.edu/~candes/teaching/math301/hand.html), Stanford University, Winter 2010-2011
- https://stanford.edu/~boyd/papers/pdf/prox_slides.pdf
- Y. Nesterov (1998), "Introductory lectures on convex optimization: a basic course", Chapter 2
- Vandenberghe, Lecture notes for EE 236 C, UCLA, Spring 2011-2012


